{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with our Multi-Layer Perceptron, we will need to import our network class as well as the provided moduele to import the mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import load_data_wrapper\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the minst data and seperate it into training data, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, _, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally create in our MultiLayer Perceptron!  We will have three layers,\n",
    "Our input layer of 784 inputs, a hidden layer of 50 neurons, and our output layer with 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([\n",
    "    784,\n",
    "    50,\n",
    "    10,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to train this network in an automated fashion, we will utilize Stochastic Gradient Descent (SGD) algorithm, which will split our training_data into small \"batches\".\n",
    "\n",
    "Each of these batches will go ahead and \"guess\" what the numbers are, then for each individual test case, we will compare the actual output to the expected output within our test_data.\n",
    "\n",
    "we will then utilize the backpropigation algorithm to calculate the gradient descent of each individual test case.  Once finished, we will then take the average of all of the results and utilize the gradient descent calculated from these numbers as a good estimate for the gradient descent for the entire dataset.\n",
    "\n",
    "We will use this estimated overall gradient desdcent to automatically adjust our weights and bias in order to achieve faster training speeds that backpropigation alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9094 / 10000, in 6.07 seconds\n",
      "Epoch 1: 9314 / 10000, in 5.96 seconds\n",
      "Epoch 2: 9394 / 10000, in 5.99 seconds\n",
      "Epoch 3: 9428 / 10000, in 5.97 seconds\n",
      "Epoch 4: 9480 / 10000, in 3.22 seconds\n",
      "Epoch 5: 9463 / 10000, in 5.80 seconds\n",
      "Epoch 6: 9478 / 10000, in 5.94 seconds\n",
      "Epoch 7: 9534 / 10000, in 5.93 seconds\n",
      "Epoch 8: 9537 / 10000, in 5.91 seconds\n",
      "Epoch 9: 9554 / 10000, in 5.93 seconds\n",
      "Epoch 10: 9532 / 10000, in 3.16 seconds\n",
      "Epoch 11: 9542 / 10000, in 5.87 seconds\n",
      "Epoch 12: 9572 / 10000, in 6.08 seconds\n",
      "Epoch 13: 9547 / 10000, in 6.19 seconds\n",
      "Epoch 14: 9575 / 10000, in 6.21 seconds\n"
     ]
    }
   ],
   "source": [
    "net.sgd(training_data, 15, 10, 3.0, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in around 95% successful identifications over a few training rounds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
